---
title: "P8105 Homework 5"
author: "Junxian Chen (jc5314)"
date: "11/2/2019"
output: 
  github_document:
    pandoc_args: --webtex

---

```{r, include = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
theme_set(theme_gray(base_size = 10) + theme(legend.position = "bottom"))
```


# Problem 1

```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

Firstly, load the dataset with missing values.

```{r}
replaceNA = function(x){
  
  if (class(x) == "numeric") {
    replace_na(x, mean(x, na.rm = TRUE))
  } else if (class(x) == "character") {
    replace_na(x, "virginica") 
  } else {
    stop("Input is neither numeric nor character")
  }

}
```

Next, a function called `replaceNA` is created. This function will take a vector as an argument, detect the input variable type and then replace missing values using the rules defined in the question. If the input value is  neither numeric nor character, the function will stop and give out error information.

Lastly, apply this function to the columns of dataset using a map statement.

```{r}
output = map_df(iris_with_missing, replaceNA)

output
```


# Problem 2

Read in and tidy the datasets:

```{r}
p2_df =
  tibble(
    file_name = list.files(path = './data', full.names = TRUE),
    data = map(file_name, ~read_csv(.)),
    group = case_when(
      file_name %>% str_detect("con") ~ "control",
      file_name %>% str_detect("exp") ~ "experiment",
    ),
    id = as.numeric(str_extract_all(file_name, "[0-9]+"))
  ) %>% 
  unnest(cols = c(data)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "data"
  ) %>% 
  mutate(
    full_id = paste(group, "_", as.character(id), sep = ""),
    week = as.numeric(str_extract_all(week, "[0-9]+"))
  ) %>% 
  select(full_id, group, id, week, data)

p2_df
```

A spaghetti plot showing observations on each subject over time:

```{r}
p2_df %>% 
  ggplot(aes(x = week, y = data, group = full_id, color = group)) +
  geom_line() +
  xlab("Week") +
  ylab("Data")
```

*Comments:* The data values of the experiment group were increasing along with time and the overall data values in experiment group were distinctly larger than the control group after 6 weeks.


# Problem 3

```{r}
generate_xy = function(beta_1, n, beta_0, sigma){
  
  tibble(
    x = rnorm(n),
    y = beta_0 + beta_1 * x + rnorm(n, 0, sigma)
  )
  
}
```

Firstly, a function called `generate_xy` is created. This function will generate *n* pairs of (*x*, *y*), where *x*'s are random numbers following Normal distribution and *y*'s come from the given model in the question.

```{r}
generate_df = function(beta_1, generate_xy, n = 30, beta_0 = 2, sigma = sqrt(50)){
  
  tibble(
    truth = beta_1,
    data = rerun(10000, generate_xy(beta_1, n, beta_0, sigma)),
    model = map(data, ~ broom::tidy(lm(y ~ x, data = .))),
    estimate = map(model, ~ .x[2,2]),
    p_value = map(model, ~ .x[2,5])
  )  %>% 
  unnest(estimate:p_value) %>% 
  select(-model, p_value = p.value)
  
}
```

Next, another function called `generate_df` is created. This function will generate 10000 datasets which contain 30 pairs of (*x*, *y*) in each. Then the generated *x* and *y* will be fitted to linear regression model and estimators of $\beta_1$ as well as p-values will be obtained. The estimators and p-values will be extracted and included in the dataset.

(1) When $\beta_1$ = 0:

```{r}
set.seed(1)

df_1 = generate_df(beta_1 = 0, generate_xy)

df_1
```

(2) When $\beta_1$ = {1, 2, 3, 4, 5, 6}:

```{r}
beta_1 = list(1, 2, 3, 4, 5, 6)

df_2 = 
  map(beta_1, ~ generate_df(.x, generate_xy)) %>% 
  bind_rows()

df_2
```

```{r}
p3_df = rbind(df_1, df_2)
```

The two datasets were combined into one dataset contains called `p3_df`.

* A plot showing the proportion of times the null was rejected (the power of the test) on the *y* axis and the true value of $\beta_1$ on the *x* axis.

```{r}
p3_df %>% 
  group_by(truth) %>% 
  mutate(
    reject_prop = sum(p_value < 0.05) / length(p_value)
  ) %>% 
  ggplot(aes(x = truth, y = reject_prop, color = truth)) +
  geom_point() + 
  geom_line() +
  xlab("True value of Beta_1") +
  ylab("Proportion of times the null was rejected") +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)
  )
```

*Comments:* Based on the plot, it can be seen that the power of the test was increased when the effect size increased. More specifically, the powers were very small when the true values of $\beta_1$ were less than 2, and the powers were large and closed to 1 when the true values of $\beta_1$ were greater than 5.

* A plot showing the average estimate of $\hat{\beta_1}$ on the *y* axis and the true value of $\beta_1$ on the *x* axis:

```{r}
plot1 = 
  p3_df %>% 
  group_by(truth) %>% 
  summarize(mean_estimator = mean(estimate)) %>% 
  ggplot(aes(x = truth, y = mean_estimator, color = truth)) +
  geom_point() + 
  geom_line() +
  xlab("True value of Beta_1") +
  ylab("Average estimate of Beta_1") +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)
  )

plot1
```

* A plot showing the average estimate of $\hat{\beta_1}$ on the *y* axis and the true value of $\beta_1$ on the *x* axis only in samples for which the null was rejected:

```{r}
null_reject_df = 
  p3_df %>% 
  filter(p_value < 0.05) %>% 
  group_by(truth) %>% 
  summarize(mean_estimator = mean(estimate)) 

plot2 = 
  plot1 + 
  geom_point(data = null_reject_df) +
  geom_line(data = null_reject_df, color = "red")

plot2
```

*Comments:* From the above plot, we can see that the sample averages of $\hat{\beta_1}$ across tests for which the null was rejected were NOT approximately equal to the true value of $\beta_1$ when 0 < $\beta_1$ < 6. Because for 0 < $\beta_1$ < 6, a $\hat{\beta_1}$ with a value smaller than the true $\beta_1$ will be close to 0 and rise a large p-value. Such an estimator with small value will not be rejected in the test. Thus, when we looking at the samples for which the null was rejected, those small estimators were exexcluded and this made the average value of remaining estimators larger than the true value.
