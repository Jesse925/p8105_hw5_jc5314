P8105 Homework 5
================
Junxian Chen (jc5314)
11/2/2019

# Problem 1

``` r
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

Firstly, load the dataset with missing values.

``` r
replaceNA = function(x){
  
  if (class(x) == "numeric") {
    replace_na(x, mean(x, na.rm = TRUE))
  } else if (class(x) == "character") {
    replace_na(x, "virginica") 
  } else {
    stop("Input is neither numeric nor character")
  }

}
```

Next, a function called `replaceNA` is created. This function will take
a vector as an argument, detect the input variable type and then replace
missing values using the rules defined in the question. If the input
value is neither numeric nor character, the function will stop and give
out error information.

Lastly, apply this function to the columns of dataset using a map
statement.

``` r
output = map_df(iris_with_missing, replaceNA)

output
```

    ## # A tibble: 150 x 5
    ##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    ##           <dbl>       <dbl>        <dbl>       <dbl> <chr>  
    ##  1         5.1          3.5         1.4         0.2  setosa 
    ##  2         4.9          3           1.4         0.2  setosa 
    ##  3         4.7          3.2         1.3         0.2  setosa 
    ##  4         4.6          3.1         1.5         1.19 setosa 
    ##  5         5            3.6         1.4         0.2  setosa 
    ##  6         5.4          3.9         1.7         0.4  setosa 
    ##  7         5.82         3.4         1.4         0.3  setosa 
    ##  8         5            3.4         1.5         0.2  setosa 
    ##  9         4.4          2.9         1.4         0.2  setosa 
    ## 10         4.9          3.1         3.77        0.1  setosa 
    ## # … with 140 more rows

# Problem 2

Read in and tidy the datasets:

``` r
p2_df =
  tibble(
    file_name = list.files(path = './data', full.names = TRUE),
    data = map(file_name, ~read_csv(.)),
    group = case_when(
      file_name %>% str_detect("con") ~ "control",
      file_name %>% str_detect("exp") ~ "experiment",
    ),
    id = as.numeric(str_extract_all(file_name, "[0-9]+"))
  ) %>% 
  unnest(cols = c(data)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "data"
  ) %>% 
  mutate(
    full_id = paste(group, "_", as.character(id), sep = ""),
    week = as.numeric(str_extract_all(week, "[0-9]+"))
  ) %>% 
  select(full_id, group, id, week, data)

p2_df
```

    ## # A tibble: 160 x 5
    ##    full_id   group      id  week  data
    ##    <chr>     <chr>   <dbl> <dbl> <dbl>
    ##  1 control_1 control     1     1  0.2 
    ##  2 control_1 control     1     2 -1.31
    ##  3 control_1 control     1     3  0.66
    ##  4 control_1 control     1     4  1.96
    ##  5 control_1 control     1     5  0.23
    ##  6 control_1 control     1     6  1.09
    ##  7 control_1 control     1     7  0.05
    ##  8 control_1 control     1     8  1.94
    ##  9 control_2 control     2     1  1.13
    ## 10 control_2 control     2     2 -0.88
    ## # … with 150 more rows

A spaghetti plot showing observations on each subject over time:

``` r
p2_df %>% 
  ggplot(aes(x = week, y = data, group = full_id, color = group)) +
  geom_line() +
  xlab("Week") +
  ylab("Data")
```

<img src="p8105_hw5_jc5314_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

*Comments:* The data values of the experiment group were increasing
along with time and the overall data values in experiment group were
distinctly larger than the control group after 6 weeks.

# Problem 3

``` r
generate_xy = function(beta_1, n, beta_0, sigma){
  
  tibble(
    x = rnorm(n),
    y = beta_0 + beta_1 * x + rnorm(n, 0, sigma)
  )
  
}
```

Firstly, a function called `generate_xy` is created. This function will
generate *n* pairs of (*x*, *y*), where *x*’s are random numbers
following Normal distribution and *y*’s come from the given model in the
question.

``` r
generate_df = function(beta_1, generate_xy, n = 30, beta_0 = 2, sigma = sqrt(50)){
  
  tibble(
    truth = beta_1,
    data = rerun(10000, generate_xy(beta_1, n, beta_0, sigma)),
    model = map(data, ~ broom::tidy(lm(y ~ x, data = .))),
    estimate = map(model, ~ .x[2,2]),
    p_value = map(model, ~ .x[2,5])
  )  %>% 
  unnest(estimate:p_value) %>% 
  select(-model, p_value = p.value)
  
}
```

Next, another function called `generate_df` is created. This function
will generate 10000 datasets which contain 30 pairs of (*x*, *y*) in
each. Then the generated *x* and *y* will be fitted to linear regression
model and estimators of
![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1 "\\beta_1")
as well as p-values will be obtained. The estimators and p-values will
be extracted and included in the dataset.

1)  When ![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1
    "\\beta_1") = 0:

<!-- end list -->

``` r
set.seed(1)

df_1 = generate_df(beta_1 = 0, generate_xy)

df_1
```

    ## # A tibble: 10,000 x 4
    ##    truth data              estimate p_value
    ##    <dbl> <list>               <dbl>   <dbl>
    ##  1     0 <tibble [30 × 2]>  0.296     0.798
    ##  2     0 <tibble [30 × 2]>  0.00648   0.996
    ##  3     0 <tibble [30 × 2]> -2.16      0.155
    ##  4     0 <tibble [30 × 2]> -0.454     0.718
    ##  5     0 <tibble [30 × 2]> -0.514     0.730
    ##  6     0 <tibble [30 × 2]>  1.71      0.243
    ##  7     0 <tibble [30 × 2]> -1.18      0.322
    ##  8     0 <tibble [30 × 2]>  1.09      0.366
    ##  9     0 <tibble [30 × 2]>  0.806     0.377
    ## 10     0 <tibble [30 × 2]>  2.35      0.131
    ## # … with 9,990 more rows

2)  When ![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1
    "\\beta_1") = {1, 2, 3, 4, 5, 6}:

<!-- end list -->

``` r
beta_1 = list(1, 2, 3, 4, 5, 6)

df_2 = 
  map(beta_1, ~ generate_df(.x, generate_xy)) %>% 
  bind_rows()

df_2
```

    ## # A tibble: 60,000 x 4
    ##    truth data              estimate p_value
    ##    <dbl> <list>               <dbl>   <dbl>
    ##  1     1 <tibble [30 × 2]>   3.03    0.0247
    ##  2     1 <tibble [30 × 2]>   1.39    0.295 
    ##  3     1 <tibble [30 × 2]>   1.93    0.103 
    ##  4     1 <tibble [30 × 2]>   0.0151  0.990 
    ##  5     1 <tibble [30 × 2]>   0.111   0.944 
    ##  6     1 <tibble [30 × 2]>  -1.24    0.366 
    ##  7     1 <tibble [30 × 2]>   1.02    0.301 
    ##  8     1 <tibble [30 × 2]>   1.82    0.223 
    ##  9     1 <tibble [30 × 2]>   2.35    0.0765
    ## 10     1 <tibble [30 × 2]>   2.46    0.0966
    ## # … with 59,990 more rows

``` r
p3_df = rbind(df_1, df_2)
```

The two datasets were combined into one dataset contains called `p3_df`.

  - A plot showing the proportion of times the null was rejected (the
    power of the test) on the *y* axis and the true value of
    ![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1
    "\\beta_1") on the *x* axis.

<!-- end list -->

``` r
p3_df %>% 
  group_by(truth) %>% 
  mutate(
    reject_prop = sum(p_value < 0.05) / length(p_value)
  ) %>% 
  ggplot(aes(x = truth, y = reject_prop, color = truth)) +
  geom_point() + 
  geom_line() +
  xlab("True value of Beta_1") +
  ylab("Proportion of times the null was rejected") +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)
  )
```

<img src="p8105_hw5_jc5314_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />

*Comments:* Based on the plot, it can be seen that the power of the test
was increased when the effect size increased. More specifically, the
powers were very small when the true values of
![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1 "\\beta_1")
were less than 2, and the powers were large and closed to 1 when the
true values of
![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1 "\\beta_1")
were greater than 5.

  - A plot showing the average estimate of
    ![\\hat{\\beta\_1}](https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta_1%7D
    "\\hat{\\beta_1}") on the *y* axis and the true value of
    ![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1
    "\\beta_1") on the *x* axis:

<!-- end list -->

``` r
plot1 = 
  p3_df %>% 
  group_by(truth) %>% 
  summarize(mean_estimator = mean(estimate)) %>% 
  ggplot(aes(x = truth, y = mean_estimator, color = truth)) +
  geom_point() + 
  geom_line() +
  xlab("True value of Beta_1") +
  ylab("Average estimate of Beta_1") +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)
  )

plot1
```

<img src="p8105_hw5_jc5314_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

  - A plot showing the average estimate of
    ![\\hat{\\beta\_1}](https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta_1%7D
    "\\hat{\\beta_1}") on the *y* axis and the true value of
    ![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1
    "\\beta_1") on the *x* axis only in samples for which the null was
    rejected:

<!-- end list -->

``` r
null_reject_df = 
  p3_df %>% 
  filter(p_value < 0.05) %>% 
  group_by(truth) %>% 
  summarize(mean_estimator = mean(estimate)) 

plot2 = 
  plot1 + 
  geom_point(data = null_reject_df) +
  geom_line(data = null_reject_df, color = "red")

plot2
```

<img src="p8105_hw5_jc5314_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />

*Comments:* From the above plot, we can see that the sample averages of
![\\hat{\\beta\_1}](https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta_1%7D
"\\hat{\\beta_1}") across tests for which the null was rejected were NOT
approximately equal to the true value of
![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1 "\\beta_1")
when 0 \< ![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1
"\\beta_1") \< 6. Because for 0 \<
![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1 "\\beta_1")
\< 6, a
![\\hat{\\beta\_1}](https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta_1%7D
"\\hat{\\beta_1}") with a value smaller than the true
![\\beta\_1](https://latex.codecogs.com/png.latex?%5Cbeta_1 "\\beta_1")
will be close to 0 and rise a large p-value. Such an estimator with
small value will not be rejected in the test. Thus, when we looking at
the samples for which the null was rejected, those small estimators were
exexcluded and this made the average value of remaining estimators
larger than the true value.0000
